{"response": "Transformers are a type of neural network architecture originally designed for natural language processing (NLP) tasks.  They process image data by dividing images into patches, treating them as a sequence, and applying self-attention mechanisms. \n", "timestamp": 1734205360.8988905, "query": "what is transformer", "context": ["VISION TRANSFORMERS\nCHAPTER 2\nLITERATURE SURVEY\n2.1 Previous Research on Transformers in Vision\nThe application of transformers in computer vision is a relatively recent development,\ngaining significanttraction sincetheintroduction of VisionTransformers(ViTs) in2020.Before\nViTs, Convolutional Neural Networks (CNNs)dominated thelandscape ofcomputer visiondue\nto their efficiency in capturing local spatial features. Early research efforts were focused on\nenhancing the hierarchical representation of images through convolutional layers. However,\nCNNs struggled withmodelingglobal dependencies, leadingresearcherstoexplorealternatives.\nThe breakthrough came with the transformer architecture, originally designed for NLP\ntasks. Dosovitskiy et al. (2020) demonstrated that transformers could process image data by\ndividing images into patches, treating them as a sequence, and applying self-attention\nmechanisms. This innovation showed thatViTs couldmatchor evenoutperform state-of-the-art\nScaling transformer models, such as Data-efficient Image Transformers (DeiT), has\nsignificantly enhanced the efficiency of trainingVision Transformers(ViTs) onsmallerdatasets\nwhilemaintaininghigh performance.\nThis breakthrough demonstrated the adaptability of transformers in\nresource-constrained scenarios.The integration of transformers with convolutional neural\nnetworks (CNNs) has given rise to hybrid models that capitalize on the strengths of both\narchitectures.\nThese models have proven particularly effective in tasks such as object detection and\nsegmentation, where combining global attentionmechanismswithlocalfeatureextractionleads\nto superiorresults.\nThese studies underscore the versatility and potential of transformers in redefining how visual\ndata isprocessedandanalyzed.\nKESHAVMEMORIALINSTITUTEOFTECHNOLOGY PAGENO.5", "A TECHNICAL SEMINARREPORTON\nVISION TRANSFORMERS\nSubmitted in partial fulfillment of requirement for the award of the degree of\nBACHELOR OF TECHNOLOGY\nIn\nCOMPUTER SCIENCE AND ENGINEERING(AI&ML)\nSubmitted by\nDULAM SATYA VAMSI KRISHNA\n21BD1A660N", "VISION TRANSFORMERS\nPRESENTATION SLIDES\nhttps://docs.google.com/presentation/d/1MrEKVmPX-80vyu02P5lPJXCo2d-guuum_Z0\nA1CBrPTg/edit#slide=id.g31cf68350cd_2_75\nKESHAVMEMORIALINSTITUTEOFTECHNOLOGY PAGENO.26", "VISION TRANSFORMERS\ncommunity, particularly in scenarios where computational power and large labeled datasets\nwere scarce. Data-efficient models like DeiT opened new possibilities for researchers focusing\non niche applications or industries with constrained resources, thus democratizing access to\ntransformer-basedvision models.\nDomain-specific adaptations ofViTs further expanded theirapplicabilityinvarious tasks\nbeyond image classification. For instance, DETR (DEtection Transformer) redefined the\napproach to object detectionbyremovingthe dependencyonhand-crafted anchorsandcomplex\npost-processing steps. DETR introduced an end-to-end transformer-based framework, where\nobjects were detected directly from image features without the need for traditional anchor\nboxes. This resulted in improved precision and recall, particularly in complex scenes with\noverlapping objects or varying scales. Similarly, SETR (SEgmentation Transformer)\ndemonstrated the ability of ViTs to achieve state-of-the-art performance in semantic\nsegmentation, where the model can identify and segment objects at a pixel level. These\nadvancements showcased ViTs' potential to excel in tasks requiring fine-grained spatial\nunderstanding, suchas medicalimageanalysis andautonomoussystems.\nIn parallel, research into improving the efficiency of transformers led to the\ndevelopment of models like Swin Transformers. Unlike traditional ViTs, Swin Transformers\nintroduced a hierarchical structure and localized attention mechanisms, which reduced the\ncomputational complexity associated with global attention. By capturing both local and global\ncontext efficiently, Swin Transformers became more suitable for high-resolution images while\nmaintaining competitive performance. This hierarchical approachallowed ViTsto handlelarger\nimages without sacrificing efficiency, opening new opportunities for applications like satellite\nimageryanalysis,medical diagnostics,and autonomousnavigationsystems.\nKESHAVMEMORIALINSTITUTEOFTECHNOLOGY PAGENO.8", "VISION TRANSFORMERS\nAdditionally, layer normalization and skip connections improve training stability and address\nthevanishinggradientproblemin deepmodels.\nFigure 1.2 Transformer Architecture\nBy leveraging this architecture, Vision Transformers offer a flexible and scalable\nsolutionfor processing imagesof varyingresolutionsandcomplexities.\n1.3 Significance of ViTs in Computer Vision\nVision Transformers are transforming the field of computervision byaddressing many\nof the limitations associated with CNNs. One of their primary advantages is their ability to\nmodel global dependencies within an image, a task where CNNs often struggle due to their\nrelianceonfixed-size convolutionalkernels.\nViTshavedemonstratedstate-of-the-art performance inseveral keyareas, including:\nKESHAVMEMORIALINSTITUTEOFTECHNOLOGY PAGENO.2"]}